\documentclass[a4paper]{article}
\usepackage{INTERSPEECH2018}

\title{
       Detecting Double-Talks (Overlapping Speech) in Conversations\\
       using Deep Learning
}

\name{
       Abdullah,
       Joachim K{\"o}hler,
       Michael Gref
}

\address{
       Fraunhofer IAIS
}

% FIXME: Set appropriate email addresses
\email{
       abdullah.motjuste@gmail.com,
       joachim.koehler@iais.fraunhofer.de,
       michael.gref@fraunhofer.de
}

\begin{document}

\maketitle

% ----------------------------------------------------------------------------------------- UTILS -
\newcommand{\outline}[1]{}  % outline notes that will not be exported.
\newcommand{\widom}[1]{}  % gudelines from https://cs.stanford.edu/people/widom/paper-writing.html

% -------------------------------------------------------------------------------------- ABSTRACT -
\begin{abstract}
\widom{
       - the problem,
       - the approach and solutions
       - the main contributions of the paper.
       - little if any background and motivation. address the experts.
       - factual but comprehensive.
}

We present a deep learning based approach for detecting overlapping speech which occur naturally during normal conversations (aka double-talk).
We evaluate appropriately training a Deep Convolutional Neural Nectwork (DCNN) in a supervised manner for the purpose,
which involves \ldots

\end{abstract}

% ----------------------------------------------------------------------------------- INDEX TERMS -
\noindent\textbf{Index Terms}:
overlapping-speech detection,
speech segmentation,
deep convolutional neural network,
re-balancing data,

% ---------------------------------------------------------------------------------- INTRODUCTION -
\section{Introduction}
\widom{
       - What is the problem?
       - Why is it interesting and important?
       - Why is it hard? (E.g., why do naive approaches fail?)
       - Why hasn't it been solved before? Or,
       - What's wrong with other solutions? How does mine differ?
       - What are the key components of my approach and results?
       - Any specific limitations of my approach?
       - Short yet detailed enough `Related Works` near the end, or make it as Section 2.
       - `Summary of Contributions` as the final para or subsection.
              - List the major contributions in bullet form,
              - Mention in which sections they can be found, doubling up as an outline.
}
% TODO: Figure out how to cite the author's names automatically

Overlapping speech, also interchangeably referred to as double-talk,
occurs when more than one speakers speak simultaneously at a given instant of time.
This is a very common occurrence in spontaneous conversations where other participants make an utterance while a speaker is already speaking with many possible intentions,
including non-competitive acknowledgement like “mhm" or reaction like laughing,
or competitive interruption like when having misjudged their turn to speak.
% In fact, the occurrence of double-talks can add to the naturalness of a conversation \cite{shriberg_spontaneous_2005,NenkovaHighFrequencyWord2008},
% and
Its occurrence
is a subject of interest for Conversation Analysis in studying the turn-taking management done by the participants in a conversation.
% ad-hoc mechanisms by which particpants manage turn-taking during a conversation.
While the frequency and duration of overlaps can vary depending on the situation,
overlaps are quite frequent and characteristically brief (predominently smaller than 1\,second)
during normal, spontaneous conversations.  % TODO: T [fig] for dt-distribution
The characteristically small duration makes precisely annotating them a very expensive manual endeavor,
while attempts to automate the process have found it to be an extremely challenging task.

The flagship scenario of most automated speech technologies is of spontaneous conversations,
and the presence of double-talks is often detrimental to their perfomance.
Speaker diarization systems, whose goal is to determine `who spoke when' in a conversation,
are penalized when they miss additional speakers in segments with overlapping speech.
In the increasingly better state-of-the-art performance of such systems,
this penalty has come to account for a major portion of the errors that remain,
so much so that Anguera et al. claim overlapping speech situations to be the `Achilles heel' of speaker diarization systems when applied to recordings of meetings \cite{anguera_speaker_2012}.
Many other speech technologies (e.g. Automatic Speech Recognition) rely on speaker homogenous segmentations produced by a speaker diarization system,
and have been reported to suffer from degraded performance in situations of double-talks \cite{cetin_speaker_2006}.

Perhaps consequently, most previous attempts at detecting natural double-talks in conversations have been made in lieu of improving speaker diarization systems.
The most successful approaches propose employing a dedicated and purely acoustic overlap detection system.
The problem has most commonly been formulated as performing frame-wise classification for the presence of either
zero (i.e. silence), one, or more than one simultaneously active speakers,
and the solutions have been implemented in a GMM-HMM based framework while engineering different combinations of acoustic features.  % TODO: T [ref] for all the main ovl in diarzn studies
In general, using additional features from blah, blah, blah, or blah % TODO: T [text] summarize other features
have been found to improve overlap detection over using only spectral features.
Geiger et al. blah blah blah  % TODO: T [text] summarize LSTM-HMM approach.
% TODO: T [search] sweep any new DL-based overlap detection in natural conversations.
The problem of overlap detection has remained unsolved
and continues to present a steep trade-off between precision and recall of the system.

The most potent source of challenges in detecting natural double-talks is rooted in the inherent imbalance between the three classes.
It can be seen in Table X that in spontaneous conversations,  % TODO: T [tbl] Add table of segment and duration proportions.
while the individual occurrences of natural overlaps constitute a significant proportion of the total number of speaker-homogenous segmentations,
due to their predominently small duration, they account for the smallest proportion of individual frames.
Some studies have attempted to solve for this issue by training their proposed systems on artificially overlapped speech.  % TODO: T [ref, search] pyknogram paper, and the recent one, and any others
Shokouhi et al. reported better results when using their proposed Pyknograms as acoustic features for detecting overlaps under different noise conditions,
but their evaluations showed a discouraging dip in performance when the overlaps were less than 2\,seconds long \cite{shokouhi_teager_2017}.
In one of the recent proposals using a Deep Learning based method,
Andrei et al. reported achieving one of the better results in detecting overlaps of 500, 100 and 25\,millisecond \textit{window durations} \cite{AndreiDetectingOverlappedSpeech2017}
(it is unclear from the paper if this is also the temporal resolution (\textit{hop-size}) at which the predictions were made)
when MFCCs were combined with other acoustic features as inputs to a convolutional neural network.
However, no evaluations of such systems have been reported on detecting naturally occurring double-talks in real conversations.
Nevertheless, we believe that this approach of artificially increasing the training data has some pitfalls.
Most such approaches use with recordings of planned speech recorded in clean conditions as the raw data.
While artificially also adding noise helps in making the system more robust for real-world scenarios,
as done by Shokouhi et al. in their study \cite{shokouhi_teager_2017},
planned speech itself has very different properties than the spontaneous scenarios the system has to be applied to.
For example, certain vocal events like laughter, or certain utterances like those used as backchannels (e.g. “hmm", “m-hm"), which often and almost exclusively occur in natural conversations as overlapping utterances,
are difficult to account for in planned speech faithfully.
Speakers exhibit very different intonations, pace, disfluencies, etc. when they are speaking alone than when they are in a conversation.
We believe that the characteristics of double-talks,
with respect to typical duration, vocalization and content,
would be difficult to replicate in artificially overlaps created from planned single speaker speech.

Therefore, for our study, we have developed and evaluated our proposed system on naturally occurring double-talks in real telephonic conversations of the Fisher Corpus.  % TODO: T [ref] fisher paper, link
We have limited ourselves to work with mono-aural audio recorded at a sample rate of 8000\,Hertz,
in part forced by our choice of the dataset,
and in part motivated by our desire for the system to be applicable to almost all recording conditions.
Our system uses a Deep Convolutional Neural Network (DCNN) based automatic feature extractor and classifier working only on log-mel-spectrograms as inputs,
and hence avoids the need of previously prevelant feature engineering necessary for the task.
We present our proposals and systematic evaluation of different training strategies for solving the class imbalance issue and other inherent challenges posed by the problem.
Furthermore, we also evaluate the imporvements to our DCNN achieved by temporally smoothing its potentially noisy predictions
by using the Viterbi algorithm that incorporates longer-term temporal patterns that our DCNN is not able to learn on its own.

% -------------------------------------------------------------------------------------- APPROACH -
\section{Approach}
Our system performs frame-wise classification of the extracted acoustic features for a mono-aural audio into three classes based on the number of simultaneously speaking speakers:
zero (silence), one, or more than one (overlap).
For our approach, we used 64\,dimensional $\text{log}_{10} \text{-scaled}$ melspectrograms (filter-banks)
extracted every 10\,milliseconds over a window of 32\,milliseconds (equivalent to 256 FFT-bins for 8000\,Hertz audio),
which have been shown to achieve better performance than MFCCs and pure spectrograms in various deep learning based acoustic models.  % TODO: T [ref] papers about fbank vs mfcc
For the purpose of reducing the mismatch between training and testing conditions, and as a crude noise reduction method,
we applied cepstral mean normalization on the extracted features.
However, we performed this on a chunk-by-chunk basis,
where the mean vector of a contiguous chunk of audio of roughly 2.5\,minutes duration was used to center all the vectors of that chunk.
This avoids the reliability issues with utterance level normalization by using long enough chunks,
while also avoiding impact of unrelated outliers if it were done over the entire datatset.
Additional variance normalization was not found to be helpful,
which is in line with other proposals using deep architectures.  % TODO: T [ref] against variance normalization in DL

The particular architecture of our DCNN is a heavily simplified version of VGG-net (Table X),  % TODO: T [ref] VGG-net
All trainings were performed using Adamax to optimize the categorical cross-entropy of the probability mass output for the three classes.
When constructing the input to our DCNN,
after the feature vectors have been normalized and before shuffled batches are created,
we add 10 frames each from immediately before and after a given frame to be classified (see inputs in Table X)  % TODO: T [tbl] DCNN arch
with the goal to provide additional contextual information to the classifier.
Our preliminary experiments showed that using fewer number of contextual frames gave relatively worse results,
while larger values came at significant computational cost.
We fixed our feature extraction pipeline and the DCNN's architecture for all our experiments in order to isolate the variables for the different training strategies discussed in later sections.

% ---------------------------------------------------------------------- TACKLING CLASS IMBALANCE -
\section{Tackling Class Imbalance}
While deep learning methods are extremely powerful,
they also suffer from degradation in performance as classic approaches do when the target classes are severely imbalanced.
There have been relatively few systematic studies to solve this problem in the context of deep learning.
Of ones that exist, most have concentrated on image classification related tasks.  % TODO: T [ref,search] DL with imbalance
Their proposals are often not so straightforward to apply on speech signals.
We discuss these approaches in the following sub-sections within the context of detecting overlapping speech.

% --------------------------------------------------------------------------------------- DATASET -
\subsection{Dataset}
%TODO: [text] mention more clearly why dataset choice helps in tackling class imbalance
%             - natural vs. artificial overlaps
%             - large dataset, affording certain decisions that would be otherwise restricted
Previous works have on detecting overlapping speech in conversations have used data from meetings in datasets like AMI, NIST RT, ICSI, and others.  % TODO: [ref] to meeting datasets
While they are appropriate, and perhaps the flagship scenario for evaluation,
these datasets pose extra challenging due to inconsistent and difficult recording conditions.
Previous works have reported to utilize only a subset of these datasets,
often to be comparable to other works,  % TODO: [ref] papers that use meeting dataset for consistency
or due to limitations imposed by these conditions,
whose size we believed was not enough to appropriately train a DCNN with different strategies to handle the class-imbalance issue.
We decided to instead perform our study using the Fisher Corpus (Part 1),  % TODO: [ref] Fisher paper
which consists of 5,850 telephone based conversations (amounting to more than 960\,hours) and is predominently used in conversational and large vocabulary speech recognition tasks in literature.  %TODO: [ref] papers using Fisher
Most of these conversations have naturally occurring double-talks,
and the shear size of the available data makes certain simpler decisions later more affordable.
However, we limited ourselves to using only roughly 200\,hours of the dataset during trainings
(groups\,001\,to\,012, with 1299\,calls) and a fixed set of roughly 91\,hours for testing
(groups\,053\,to\,058, with 551\,calls),
mainly restricted by the training times for a DCNN.

Nevertheless, the Fisher Corpus imposes certain limitations of its own.
Being a telephone based conversation, the maximum number of overlapping speakers is strictly limited to two.
However, this scenario covers the majority of double-talk situations even when there are more than two participants.  % TODO: [ref] Zelenak
All audios are sampled at 8,000\,Hertz, which could be a disadvantage,
but it is easier to reliably downsample audios of higher quality than doing the opposite.
In fact, by using mel-scaling with a fixed number of 64\,filter-banks,
our system can injest audio of any sample-rate,
although it may not be able to benefit from the better characteristics of higher quality audios.
Along the same lines, the raw recordings in the Fisher Corpus have two speaker specific channels which, when merged,
can only be used as mono-aural audios.
While using multi-channel audio could be better for performance,
but by supporting this least common denominator condition,
our system is applicable to most scenarios.

The most important problem with the dataset is the relative imprecision in annotation of the utterance boundaries.
As per the documentation,
these boundaries were not manually modified after they were determined by a speech activity detection system.   % TODO: [ref] Fisher corpus
The properties of this speech activity detection system have not been mentioned,
but we found them to be less precise than ideal, especially in regions of small gaps.
These inaccuracies do not impact ASR systems as much as they would a system that needs to temporally localize events in the audio.
A powerful enough DCNN, when a large enough dataset is available to learn from,
may however be robust to such lack of precision,
and we did indeed observe this in many of our experiments.
Such inaccuracies exist in other datasets as well,
but they do not offer as much data for a deep learning method to learn from comfortably,
motivating us further to use the Fisher Corpus.
Nonetheless, these inaccuracies can adversely impact the summary metrics used for evaluating the system, including the ones reported here.
We did not, however, modify the annotations before using them for the experiments reported here,
but we strongly recommend using a better speech activity detector to improve the available utterance boundaries for future works in this direction.


% ---------------------------------------------------------------------------------- BIBLIOGRAPHY -
\bibliographystyle{IEEEtran}
\bibliography{dt-paper}
\end{document}
