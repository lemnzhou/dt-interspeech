\documentclass[a4paper]{article}
\usepackage{INTERSPEECH2018}

\title{
       Detecting Overlapping Speech Segments in Real Conversations\\
       Using Deep Learning
}

\name{
       Abdullah,  % $^1$,
       Joachim K{\"o}hler,  % $^2$,
       Michael Gref  % $^2$
}

\address{
      % $^1$RWTH Aachen, Germany \\
      % $^2$
      % Fraunhofer IAIS, Germany
      Fraunhofer Institute for Intelligent Analysis and Information Systems IAIS, Germany
}

% FIXME: Set appropriate email addresses
\email{
       abdullah.abdullahe@rwth-aachen.de,
       \{joachim.koehler, %@iais.fraunhofer.de,
       michael.gref\}@iais.fraunhofer.de
}

\begin{document}

\maketitle

% ----------------------------------------------------------------------------------------- UTILS -
\newcommand{\outline}[1]{}  % outline notes that will not be exported.
\newcommand{\widom}[1]{}  % gudelines from https://cs.stanford.edu/people/widom/paper-writing.html

% -------------------------------------------------------------------------------------- ABSTRACT -
\begin{abstract}
\widom{
       - the problem,
       - the approach and solutions
       - the main contributions of the paper.
       - little if any background and motivation. address the experts.
       - factual but comprehensive.
}
Segmenting natural occurrences of overlapping speech in real conversations has proven to be an extremely difficult challenge, demanding heavy feature engineering, or controlled studies using artificial data.
We demonstrate training a Deep Convolutional Neural Network (DCNN) for this task, affording us to use relatively low-level log-scaled Mel-spectrograms from only mono-aural audio, one of the most difficult scenarios for this task.
We propose using the real conversational data from the Fisher English Corpus to properly train the DCNN while maintaining and testing its generalizability to real conversational scenarios.
To alleviate the then imposed challenge of severe class-imbalance, we report the improvements achieved by removing silence from the training objective and uniformly randomly under-sampling the majority class during training.
Simultaneously, we perform temporal smoothing using the Viterbi algorithm to enhance the final segmentation.
With more than 60\%~precision and more than 29\%~recall over more than 91~hours of conversations, our results demonstrate the applicability of deep learning for this task and, using our modular and open-sourced RENNET framework,
straightforward avenues for future works in this direction.

\end{abstract}

% ----------------------------------------------------------------------------------- INDEX TERMS -
\noindent\textbf{Index Terms}:
overlapping-speech,
deep convolutional neural network,
conversation analysis,
speech segmentation,
class-imbalance,
deep learning.

% ---------------------------------------------------------------------------------- INTRODUCTION -
\section{Introduction}
\widom{
       - What is the problem?
       - Why is it interesting and important?
       - Why is it hard? (E.g., why do naive approaches fail?)
       - Why hasn't it been solved before? Or,
       - What's wrong with other solutions? How does mine differ?
       - What are the key components of my approach and results?
       - Any specific limitations of my approach?
       - Short yet detailed enough `Related Works` near the end, or make it as Section 2.
       - `Summary of Contributions` as the final para or subsection.
              - List the major contributions in bullet form,
              - Mention in which sections they can be found, doubling up as an outline.
}

Overlapping speech, interchangeably referred to as double-talk in this paper,
occurs when more than one speakers speak simultaneously at a given instant.
It is a common occurrence in spontaneous conversations when
other participants often utter something while a speaker is already speaking,
possibly to demonstrate non-competitive acknowledgement (e.g. â€œmhm") or reaction (e.g. laughing),
or competitively interrupt having misjudged their turn to speak.
Its occurrence is of interest for Conversation Analysis in studying the
turn-taking management done by the participants of a conversation.
While the frequency and durations of overlaps can vary depending on the situation,
overlaps are quite frequent and characteristically brief (predominently smaller than 1~second)
during normal spontaneous conversations (Figure~\ref{fig:dt-dist}),
making their manual annotation expensive and automatic detection challenging.

In the flagship scenario of spontaneous conversations,
the presence of double-talks is often detrimental to most automated speech technologies.
Speaker diarization systems, whose goal is to determine `who spoke when' in a conversation,
are penalized when they miss additional speakers during overlaps.
This penalty has become a major portion of the errors that remain in the
state-of-the-art performance of such systems,
so much so that Anguera et al. claimed overlapping speech situations to be the `Achilles heel' of
speaker diarization systems when applied to meetings \cite{anguera_speaker_2012}.
Many other speech technologies (e.g. Automatic Speech Recognition)
rely on the speaker homogenous segmentations produced by a diarization system,
and have been reported to suffer from degaradation as well \cite{cetin_speaker_2006,RenalsDistantspeechrecognition2017}.

% ---------------------------------------------------------------------------------- FIG:DT-DIST -
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/dt-dist.png}
  \caption{Weighted histogram of durations of segments with double-talk in the Fisher English Corpus (Part 1).}
  \label{fig:dt-dist}
  \vspace*{-\baselineskip}
\end{figure}

Perhaps consequently, most previous attempts at detecting natural double-talks in real conversations
have been made in the speaker diarization community.
The problem has most commonly been formulated as performing frame-wise classification
for the presence of either
zero (i.e. silence), one, or more than one simultaneously active speakers,
and the solutions have been implemented in a GMM-HMM framework
while engineering different combinations of acoustic features.
In general, using additional, higher-level features like
diarization posteriors' entropy \cite{boakye_overlapped_2008},
TDOA cross-correlation from mutliple microphones \cite{zelenak_simultaneous_2012},
prosodic attributes \cite{zelenak_speaker_2012},
energy ratios of speakers' CNSC bases \cite{vipperla_speech_2012,geiger_speech_2012-1},
linguistic content \cite{GeigerUsinglinguisticinformation2013},
multi-pitch detection \cite{charlet_impact_2013},
and long-term statistics of gaps and speaker-changes \cite{yella_overlapping_2014}
have been found to improve overlap detection (and consequently diarization)
over using only spectral features like MFCC, RMS, LPC, spectral flatness, flux, kurtosis, etc.
alone or in different combinations, when evaluated on different meeting corpora.
In early uses of deep learning,
Zelen\'{a}k et al. employed a DNN for dimensionality reduction of their TDOA cross-correlation features \cite{zelenak_simultaneous_2012},
while Geiger et al. used an LSTM-based regressor,
which was trained on various acoustic features, in a tandem setting \cite{geiger_detecting_2013},
both effectively using them as additional features to a GMM-HMM based detector.
Geiger et al., however, also reported that their LSTM alone performed comparably to the GMM-HMM by simply thresholding the regressor's outputs \cite{geiger_detecting_2013}.
Nevertheless, the problem of overlap detection in real conversations has remained unsolved and
presents a characteristically steep trade-off between precision and recall of the system.

% ------------------------------------------------------------------------------- TBL:ACTSPK-PERC -
\begin{table}[t]
  \caption{Percentage of segments and frames with different number of simultaneously active speakers in the $\sim \!\!\! \text{961~hours}$ of Fisher English Corpus (Part 1), demonstrating the imbalance.}
  \centering
  \begin{tabular}{crr}
    \toprule
    \textbf{Active Speakers}  & \textbf{Segments (\%)}  & \textbf{Frames (\%)}  \\ \midrule
    0 -- $(no)$               & 20.39                   &  6.86                 \\
    1 -- $(sp)$               & 50.93                   & 79.61                 \\
    2 -- $(ov)$               & 28.68                   & 13.53                 \\
    \bottomrule
  \end{tabular}
  \label{tbl:actspk-perc}
  \vspace*{-\baselineskip}
\end{table}

The most potent source of challenges in learning to detect double-talks from real conversations
is rooted in the inherent imbalance between the three classes.
Demonstrated in Table~\ref{tbl:actspk-perc}, in spontaneous conversations,
while the individual segments with overlaps constitute a significant proportion of the total,
due to their predominently small duration,
they account for the smallest proportion of individual (10~ms) frames.
Some studies have attempted to solve this issue by training on artificially overlapped speech.
Shokouhi et al. reported better results when using their proposed Pyknograms as acoustic features for detecting overlaps under different noise conditions,
but their evaluations showed a discouraging dip in performance when the overlaps were less than 2~seconds long \cite{shokouhi_teager_2017}.
Similarly limitated temporal resolutions were reported by Andrei et al. for their previous works on a related task,
but in one of their recent proposals using a Deep Learning based method,
Andrei et al. reported achieving one of the better results in detecting overlaps of
500,~100~and~25~ms \textit{window durations} \cite{AndreiDetectingOverlappedSpeech2017}
(it is unclear from the paper if this is also the temporal resolution, i.e. the \textit{hop-size}, at which the predictions were made, neither is the distribution of the lengths of overlaps).
They combined MFCC with three other acoustic features to train a convolutional neural network.
However, no evaluations of such systems have been reported on detecting natural double-talks in real conversations,
while our own exploratory ones were not encouraging.

We believe that this approach of artificially increasing the training data for
this task has some significant pitfalls.
Most such approaches use recordings of planned speech recorded in clean conditions as the raw data.
While artificially adding noise may help with robustness for real-world scenarios
(as done by Shokouhi et al. in their study \cite{shokouhi_teager_2017}),
planned speech itself has very different properties than
the spontaneous scenarios the system has to be ultimately applied to.
For example, certain vocal events like laughter,
or certain utterances like those used as backchannels (e.g. â€œhmm", â€œm-hm"),
which often and almost exclusively occur in natural conversations as overlapping utterances \cite{GeigerUsinglinguisticinformation2013},
are difficult to account for in planned speech faithfully.
Speakers exhibit very different intonations, pace, disfluencies, etc. when
they are speaking alone than when they are in a spontaneous conversation.
We believe that the characteristics of double-talks,
with respect to typical duration, vocalization and content,
would be difficult to replicate in artificial overlaps created from planned single speaker speech.

Therefore, we have developed and evaluated our proposed system on real conversations with natural double-talks.
It uses a DCNN based automatic feature extractor and classifier working only on
log-scaled Mel-spectrograms as inputs,
and hence avoids the need for manual feature engineering
(Section~\ref{sec:approach}).
We limited ourselves to work with mono-aural audios with sampling-rate of 8~kHz,
partly forced by our choice of the dataset,
but mainly motivated by our desire for the system to be applicable to almost any recording setup
(Section~\ref{sec:dataset}).
Here we describe effective training strategies for counteracting the class-imbalance
(Section~\ref{sec:imbalance}) and evaluate their impact (Section~\ref{sec:eval}).
% alongside the benefits of temporal smoothing using Viterbi algorithm .

% -------------------------------------------------------------------------------------- APPROACH -
\section{Approach} \label{sec:approach}
Our system performs frame-wise classification of the extracted acoustic features for a mono-aural audio into three classes based on the number of simultaneously speaking speakers:
zero~(silence)~$(no)$, one~$(sp)$, or more than one~(overlap)~$(ov)$.
We used 64~dimensional $\text{log}_{10} \text{-scaled}$ Mel-spectrograms (filter-banks)
extracted every 10~ms over a window of 32~ms (equivalent to 256~FFT-bins for 8~kHz audio),
which have been shown to be better than MFCCs and pure spectrograms
for various deep learning based acoustic models \cite{deng_recent_2013}.
For the purpose of reducing the mismatch between training and testing conditions,
and, given the features are log-scaled, as a measure against convolutive channel distortions \cite{li_overview_2014},
we applied Cepstral Mean Normalization (CMN) on the extracted features.
However, we performed this on a chunk-by-chunk basis,
where the mean vector of a roughly 2.5~minutes long contiguous chunk of audio of was used to center all the vectors of that chunk.
This avoids the reliability issues with utterance level CMN \cite{prasad_improved_2013}
since the chunks are longer,
while also avoiding any impact of unrelated outliers if it were done over the entire datatset.

% --------------------------------------------------------------------------------- TBL:DCNN-ARCH -
\begin{table}[t]
  \caption{Architecture of the DCNN, and layers in each block.}
  \centering
  \begin{tabular}{ll}
    \toprule
    \textbf{Block}    & \textbf{Output's Shape}                         \\
                      & \tiny{(time, frequency, channel) or (size, )}   \\ \midrule
    Inputs            & (21, 64, 1)                                     \\ \midrule
    ConvBlk 1         & (9, 31, 64)                                     \\
    ConvBlk 2         & (3, 14, 128)                                    \\
    ConvBlk 3         & $\text{(256, )}^{\text{\tiny{global pooling}}}$ \\ \midrule
    FCBlk 1           & (512, )                                         \\
    FCBlk 2           & (128, )                                         \\
    FCBlk 3           & (32, )                                          \\ \midrule
    Outputs           & $\text{(3, )}^{\text{\tiny{softmax}}}$          \\
    \bottomrule
  \end{tabular}
  \begin{footnotesize}
    \begin{tabular}{l}
      \toprule
      \textbf{ConvBlk}   \\ \midrule
      Conv2D: (3, 3)     \\
      BatchNorm          \\
      ReLU               \\
      Dropout:  0.1      \\
      MaxPool2D: (2, 2)  \\
      \bottomrule
    % \end{tabular}
    % \begin{tabular}{l}
                         % \\
      \toprule
      \textbf{FCBlk}     \\ \midrule
      Dense              \\
      ReLU               \\
      Dropout:  0.1      \\
      \bottomrule
    \end{tabular}
  \end{footnotesize}
  \label{tbl:dcnn-arch}
  \vspace*{-\baselineskip}
\end{table}

The architecture of our DCNN is a heavily simplified version of VGG-net \cite{simonyan_very_2014},
with roughly 572k trainable parameters (Table~\ref{tbl:dcnn-arch}).
All trainings were performed after a Glorot uniform initialization and by using Adamax to optimize the categorical cross-entropy of the output posterior probabilites for the three classes.
When constructing an input to our DCNN,
after the feature vectors have been normalized but before shuffled mini-batches are created,
we add 10 frames each from immediately before and after a given frame that is to be classified
to provide additional contextual information to the classifier.
Using fewer of such contextual frames lead to worse results,
while using more came at significant computational cost.
We fixed our feature extraction pipeline and the DCNN's architecture for all our experiments,
implemented using our highly modular framework for
speech segmentation using deep learning open-sourced as
{RENNET}\footnote{https://github.com/fraunhofer-iais/rennet},
which is built on top of NumPy, LibROSA, Keras, Tensorflow, and other reputable Python libraries.

% As it is, the DCNN does not exploit the longer-term temporal patterns inherent to this problem
% (e.g. typical duration of segments, probability of occurrence, etc.).
The `raw' segmentation by simple frame-wise $argmax$ of the posteriors from the DCNN are usually noisy.
We used the Viterbi algorithm \cite{rabiner_tutorial_1989} for decoding a temporally `smoothed' final segmentation and evaluate its impact here (Figure~\ref{fig:raw-smth-preds}).
The prior- and transition-probabilties were calculated on the training set and,
being class posteriors, the emission-probabilties were set to $1$.
Unlike many previous works \cite{zelenak_simultaneous_2012,zelenak_speaker_2012,GeigerUsinglinguisticinformation2013,yella_overlapping_2014,geiger_detecting_2013} however,
we did not use an overlap-insertion penalty or manually modify the calculated probabilities.

% ---------------------------------------------------------------------------- FIG:RAW-SMTH-PREDS -
\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/raw-smth-preds.png}
  \caption{True labels, raw-, and smoothed-predictions for part of a conversation, after training on rebalanced data without silence.}
  \label{fig:raw-smth-preds}
  \vspace*{-\baselineskip}
\end{figure*}

% --------------------------------------------------------------------------------------- DATASET -
\section{Dataset} \label{sec:dataset}
Previous works on detecting overlapping speech in conversations have used AMI, NIST RT, ICSI,
and other meeting datasets (all mentioned in \cite{cetin_speaker_2006}).
While they are appropriate, and perhaps the flagship scenario for this problem,
these datasets pose extra challenges due to inconsistent and difficult recording conditions.
Furthermore, previous works have utilized only a subset of these datasets,
often to stay comparable to other works or due to other limitations.
This subset was not enough to appropriately train a DCNN while tackling class-imbalance.
We instead performed our study using the Fisher English Corpus (Part 1) \cite{cieri_switchboard_2003},
which is predominently used in LVCSR research.
Most of its telephone based conversations have naturally occurring double-talks,
and the shear size ($\sim \! \text{961~hours}$) of the available data makes certain simpler decisions later more affordable.
However, we only used $\sim \! \text{200~hours}$
($\sim \! \text{10\%}\,(ov)$) of the dataset during trainings
(groups~001~to~012, with 1299~calls) and a fixed set of
$\sim \! \text{91~hours}$ ($\sim \! \text{14\%}\,(ov)$) for testing
(groups~053~to~058, with 551~calls),
mainly restricted by the computational cost.

Nevertheless, the Corpus imposes certain limitations of its own.
Being a telephone based conversation,
the maximum number of overlapping speakers is strictly limited to two.
However, this scenario covers the majority of double-talk situations even when there are more than two participants.
All audios are sampled at 8~kHz, which could be a disadvantage,
but it is easier to reliably downsample audios of higher quality than doing the opposite.
In fact, by using mel-scaling with a fixed number of filter-banks,
our system can ingest audio of any sample-rate.
Along same lines,
the raw audios have two speaker specific channels which, when merged,
can only be used as mono-aural audios.
While using multi-channel audio could be better for performance,
by supporting this least common denominator condition,
our system is applicable to almost all scenarios.

A more pertinent problem with the Corpus is the relative imprecision in the utterance boundaries.
As per the documentation,
these boundaries were determined by a speech activity detection system and not manually verified,
but the properties of this system have not been mentioned,
and we found it to be less precise than ideal, especially in regions of small gaps.
These inaccuracies do not impact ASR systems as much as they would a system that needs to temporally localize events in the audio.
A powerful enough DCNN trained on a large dataset can be robust against such lack of precision,
and we did indeed observe this in many of our experiments.
Nevertheless, these inaccuracies can adversely impact the evaluations of the system.
We did not modify the annotations before using them for the experiments reported here,
but improving them should be easier given independent speaker channels are available.

% ---------------------------------------------------------------------- TACKLING CLASS IMBALANCE -
\section{Tackling class-imbalance} \label{sec:imbalance}
% \vspace*{-\baselineskip}
% -------------------------------------------------------------------- RE-BALANCING TRAINING DATA -
\subsection{Rebalancing training data}
% \vspace*{-\baselineskip}
Rebalancing the data that is seen by the system during training is the most common approach used to tackle the impact of imbalanced classes,
by either oversampling from the minority classes or undersampling from the majority classes
\cite{wang_training_2016,Budasystematicstudyclass2017}.
When the dataset is small, oversampling is the most prevelant method,
and is preferably done by creating new examples of the minority class by transforming existing ones using several methods to avoid overfitting (e.g. SMOTE).
However, such transformations are not straightforward for audio signals.
Nevertheless, afforded by the size of our dataset,
we instead chose to only evaluate majority undersampling for one of our experiments reported here.
For this, we skipped frames with a single speaker's speech uniformly randomly when creating the mini-batches,
such that the final overall $(sp)$ to $(ov)$ ratio was two-to-one.
We chose this ratio as an approximate method for presenting the DCNN with relatively equal number of frames for each speaker alone and as overlapped.
Undersampling while preferring examples near the class boundaries may help learn more discriminating features,
but we did not do this due to the imprecise boundary annotations in most speech datasets.

% ------------------------------------------------------------------------------ REMOVING SILENCE -
\subsection{Removing silence} \label{sec:rm-silence}
% \vspace*{-\baselineskip}
Silence, or lack of speech, is more easily discriminable than speech from any number of speakers,
whereas discriminating between speech produced by a single speaker and that produced by multiple speakers simultaneously, even in isolation, can prove difficult.
This problem becomes more severe due the imbalance disadvantaging overlapping speech more,
and may result in a neural network getting stuck in a steep local minima where its objective for detecting silence is met very well, but not at all for double-talk detection.
Furtheremore, silence detection can be done by using much simpler methods than an expensive DCNN.
Therefore, similar to most previous works,
we experimented with removing all silence frames during training based on the true labels.
To enable temporal smoothing during evaluations of these experiments,
we explicitly set the silence frames to be perfect predictions based on the true labels for silence.

% ----------------------------------------------------------------------------------------- OTHER -
\subsection{Other}
% \vspace*{-\baselineskip}
Moving the decision threshold based on class priors is another popular method,
but our experiments with this method were unstable and unreliable enough to be inconclusive,
and hence they are not reported here.
Perhaps this was due to the imbalance being too steep over a relatively few number of classes.
Another method is to modify the loss function to incorporate misclassification penalties.
However, it is equivalent to naive oversampling for stochastic gradient descent based training \cite{Budasystematicstudyclass2017}.

% ----------------------------------------------------------------------------------- EXPERIMENTS -
\section{Experiments} \label{sec:eval}

% ---------------------------------------------------------------------------------- TBL:RESULTS -
\begin{table}[t]
  \caption{Class-wise precision $P$ and recall $R$ after different strategies, using $argmax$ `raw' or smoothed `smth` decoding.}
  \centering
  % \begin{tabular}{rrrrrrr}
  \begin{tabular}{ccccccc}
    \toprule
                & \multicolumn{2}{c}{\textbf{Baseline}}  & \multicolumn{2}{c}{\textbf{w/o Silence}}   & \multicolumn{2}{c}{\textbf{Rebalanced}}   \\
                &         raw      &         smth        &         raw      &         smth            &         raw      &         smth           \\ \midrule
    $P^{(no)}$  &         19.43    &         20.90       &         --       &         --              &         --       &         --             \\
    $R^{(no)}$  &         88.09    &         89.10       &         --       &         --              &         --       &         --             \\ \midrule
    $P^{(sp)}$  &         86.45    &         85.92       &         86.83    &         86.48           &         89.12    &         88.91          \\
    $R^{(sp)}$  &         82.15    &         83.61       &         99.05    &         99.79           &         88.39    &         96.69          \\ \midrule
    $P^{(ov)}$  & \textbf{81.30}   & \textbf{91.76}      & \textbf{68.87}   & \textbf{88.13}          & \textbf{35.25}   & \textbf{60.46}         \\
    $R^{(ov)}$  & \textbf{ 7.89}   & \textbf{ 5.05}      & \textbf{12.22}   & \textbf{ 8.82}          & \textbf{36.93}   & \textbf{29.55}         \\
    \bottomrule
  \end{tabular}
  \label{tbl:results}
  \vspace*{-\baselineskip}
\end{table}

In the presence of imbalance, classwise precision and recall are more informative than common measures like overall accuracy.
We report these metrics of the raw and smoothed segmentations at the frame level for all classes.
All evaluations were performed on the roughly 91~hours of conversations specified earlier.

In our preliminary experiments with several deep learning architectures and parameters,
we observed certain degenerate cases where the system failed to satisfactorily detect $(ov)$.
Some of the least powerful architectures predicted all fames to only be $(sp)$,
often with posterior probability as $1$, where not even temporal-smoothing could have helped.
% This is the classic case where a metric like overall accuracy would have been misleading.
A common degenerate case with moderately powerful architectures was where the system would perform robustly in detecting $(no)$ but will never detect any $(ov)$.
This unexpected voiced-activity detector demonstrated the extra disadvantage $(ov)$ face when $(no)$ is kept during training.
When some measures like threshold-moving were applied,
a common degenerate case involved extremely low precision for $(ov)$ ($P^{(ov)}$) from what seemed like random decisions for a frame to be $(sp)$ or $(ov)$.
% Interestingly, in cases where silence was kept as part of the training objective,
% this catastrophic level confusion still almost exclusively occurred between $(sp)$ and $(ov)$,
% indicated by both a low $P^{(ov)}$ and lower than 70\% recall for $(sp)$ ($R^{(sp)}$).

For a baseline performance measure of our DCNN,
we trained the system for 20~passes over the training set without rebalancing the classes and also kept silence as part of the training objective.
As tabulated in Table~\ref{tbl:results},
this baseline system did not detect $(no)$ with good precision ($P^{(no)}$).
This proved taxing for detecting $(ov)$, resulting in unsatisfactory recall ($R^{(ov)}$),
perhaps due to the increased competition from silence in varying noise conditions.
However, the good $P^{(ov)}$ indicates that the DCNN did indeed find some of the discriminative features for $(ov)$.
Temporal-smoothing improved $P^{(no)}$ and $P^{(ov)}$ by either removing impossibly short segments from the raw predictions or by filling in certain gaps,
but this also removed some correctly predicted frames, mostly reducing $R^{(ov)}$.

We then trained the same DCNN for 20 passes over the same imbalanced training set,
but with the silence frames removed.
During evaluation, the silence frames were not removed from the input but,
interestingly enough, over the entire testing set,
none of these silence frames were ever detected as $(ov)$,
strongly suggesting that the DCNN had learned features that were unique to $(ov)$.
For raw segmentation, we observed improvements in $R^{(ov)}$ but at a significant cost to $P^{(ov)}$.
Temporal-smoothing however regained the lost $P^{(ov)}$ with a relatively smaller cost to $R^{(ov)}$ than in the previous experiment,
all the while not cannibalizing on performance for $(sp)$.

For the final experiment presented here,
we trained our DCNN for 40 passes with uniformly randomly undersampled $(sp)$ as explained earler, while also removing silence.
We doubled the number of passes for this experiment to avoid any underfitting of the single-speaker class.
The evaluation on the testing set (which was obviously not rebalanced) showed an encouraging improvement in $R^{(ov)}$,
but with discouragingly high number of false positives in the raw predictions.
However, most of these false positives occurred as noisy, impossibly short segments,
and were very adequately removed by the temporal smoothing procedure (Figure~\ref{fig:raw-smth-preds}),
leading to a substantial improvement in $P^{(ov)}$ at a still smaller relative cost to $R^{(ov)}$.

% ----------------------------------------------------------------------------------- CONCLUSIONS -
\section{Conclusions}
We have demonstrated training a DCNN on real conversations to detect natural double-talks.
Recognizing the difficulties posed by the traditionally used meeting datasets for this task,
we proposed using the Fisher English Corpus instead of artificially overlapped data for training.
While the added limitation to only mono-aural audios at low sampling rate makes it one of the most technically difficult scenario for this task,
it also makes the resulting system the most versatile in applicability.
Using real conversations however poses the challenge of class-imbalance that severely disadvantages double-talks.
However, even without tackling it, our DCNN learned some features unique to double-talks.
In fact, our DCNN automatically learned these features only from the relatively low-level log-scaled Mel-spectrograms,
as opposed to manually engineered combinations used by previous works.

We showed that, in line with previous works,
removing silence from the training objective is helpful in reducing the disadvantage against double-talks.
% A good voiced-activity detector can be straightforwardly used in a preprocessing step during application phase for this task.
We then showed that much more improvement can be achieved by rebalancing the classes during training.
The size of the Corpus afforded us to choose undersampling as the strategy for this purpose.
Future works with this Corpus for this task should consider employing a better speech detector to improve the precision of the annotations.

There is obviously still a large room for improvement.
We expect a more powerful DCNN architecture to perform better,
but we also demonstrated the weakness of DCNNs in exploiting longer-term temporal patterns,
and reported improvements by temporal-smoothing of the posteriors using the Viterbi algorithm.
We therefore expect components that are capable of learning such patterns automatically to be more worthwhile modifications to investigate (e.g. LSTM cells).
There is a lack of comprehensive studies for tackling class imbalance when using deep learning,
especially in tasks related to speech technologies.
We believe that this task could be a worthwhile candidate for such studies,
if not only benefitting speech technologies that suffer in situations of overlapping speech.

% ------------------------------------------------------------------------------ ACKNOWLEDGEMENTS -
\section{Acknowledgements}
This research was funded by the Federal Ministry of Education and Research of Germany (BMBF) in the project \textit{KA$^3$~-~K{\"o}lner Zentrum fÃ¼r Analyse und Archivierung von AV-Daten} (Cologne center for the analysis and archiving of audiovisual data), number:~01UG1511B.
Felix Rau (Universit{\"a}t zu K{\"o}ln) and Alexander Esser (Fraunhofer IAIS) also contributed invaluably.

% ---------------------------------------------------------------------------------- BIBLIOGRAPHY -
\bibliographystyle{IEEEtran}
\bibliography{dt-paper}
\end{document}
